# @package _global_
model:
  _name_: transformer
  d_model: 288
  dim: ${model.d_model} # temporary hack to make llama happy
  d_inner: ${eval:'${model.d_model} * 4'}
  n_layer: 6
  n_heads: 8
  n_kv_heads: ${model.n_heads}
  multiple_of: 32 # make SwiGLU hidden layer size multiple of large power of 2
  max_seq_len: 256
  dropout: 0.0
  norm_eps: 1.0e-5
